{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13256575,"sourceType":"datasetVersion","datasetId":8400297}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run this cell to check if GPU is available\nimport tensorflow as tf\n\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n\n# For PyTorch\nimport torch\nprint(\"CUDA Available:\", torch.cuda.is_available())\nprint(\"CUDA Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install imbalanced-learn==0.12.0 -q\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install \n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install xgboost -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install scikit-learn -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, \n                             f1_score, classification_report, confusion_matrix)\n# SMOTE removed due to compatibility issues - using class_weight='balanced' instead\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\n\n# Set plotting style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\nprint(\"✓ All libraries imported successfully!\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Scikit-learn version: {sklearn.__version__}\")\nprint(\"\\n⚠ Note: Using class_weight='balanced' instead of SMOTE for class imbalance\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:29:07.279834Z","iopub.execute_input":"2025-10-04T20:29:07.280081Z","iopub.status.idle":"2025-10-04T20:29:07.287415Z","shell.execute_reply.started":"2025-10-04T20:29:07.280058Z","shell.execute_reply":"2025-10-04T20:29:07.286527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 2: CONFIGURE PATHS AND PARAMETERS\n# ====================================================================\n\n# Configuration\nCONFIG = {\n    # File paths - UPDATE THESE WITH YOUR KAGGLE DATASET PATHS\n    'kepler_path': '/kaggle/input/spacedata/cumulative_2025.10.03_22.35.18.csv',\n    'tess_path': '/kaggle/input/spacedata/TOI_2025.10.03_22.38.09.csv',  # Add path if you have TESS data\n    'k2_path': '/kaggle/input/spacedata/k2pandc_2025.10.03_22.38.28.csv',    # Add path if you have K2 data\n    \n    # Model parameters\n    'test_size': 0.2,\n    'random_state': 42,\n    'use_smote': True,\n    \n    # Random Forest parameters\n    'n_estimators': 200,\n    'max_depth': 20,\n    'min_samples_split': 5,\n    'min_samples_leaf': 2,\n    \n    # Output\n    'save_model': True,\n    'model_name': 'exoplanet_detector',\n}\n\nprint(\"✓ Configuration loaded\")\nprint(f\"  Kepler data: {CONFIG['kepler_path']}\")\nprint(f\"  TESS data: {CONFIG['tess_path']}\")\nprint(f\"  K2 data: {CONFIG['k2_path']}\")\nprint(f\"  Test size: {CONFIG['test_size']}\")\nprint(f\"  Random state: {CONFIG['random_state']}\")\nprint(f\"  Use SMOTE: {CONFIG['use_smote']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:29:12.314338Z","iopub.execute_input":"2025-10-04T20:29:12.315206Z","iopub.status.idle":"2025-10-04T20:29:12.321703Z","shell.execute_reply.started":"2025-10-04T20:29:12.315173Z","shell.execute_reply":"2025-10-04T20:29:12.320716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Auto-detect available data files\nimport os\nprint(\"Available datasets:\")\nfor root, dirs, files in os.walk('/kaggle/input/'):\n    for file in files:\n        if file.endswith('.csv'):\n            full_path = os.path.join(root, file)\n            size = os.path.getsize(full_path) / 1024**2\n            print(f\"  {full_path}\")\n            print(f\"    Size: {size:.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:29:15.994263Z","iopub.execute_input":"2025-10-04T20:29:15.994838Z","iopub.status.idle":"2025-10-04T20:29:16.026502Z","shell.execute_reply.started":"2025-10-04T20:29:15.994816Z","shell.execute_reply":"2025-10-04T20:29:16.025802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 3: LOAD DATA\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"LOADING DATA\")\nprint(\"=\"*70)\n\n# Load Kepler data\ntry:\n    print(\"Loading Kepler data (this may take a moment)...\")\n    \n    # NASA Exoplanet Archive files start with comment lines (#)\n    # We need to skip these comment lines\n    df = pd.read_csv(\n        CONFIG['kepler_path'],\n        comment='#',           # Skip lines starting with #\n        low_memory=False,      # Read entire file at once\n        encoding='utf-8'       # Specify encoding\n    )\n    \n    print(f\"✓ Successfully loaded Kepler data\")\n    print(f\"  Shape: {df.shape}\")\n    print(f\"  Columns: {df.shape[1]}\")\n    print(f\"  Rows: {df.shape[0]:,}\")\n    print(f\"  Memory: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n    \n    # Check if we actually got data\n    if df.shape[0] < 100:\n        print(\"\\n⚠ WARNING: Very few rows loaded. The file might have issues.\")\n        print(\"Checking if this is the right file...\")\n        \n        # Try to peek at the file structure\n        with open(CONFIG['kepler_path'], 'r') as f:\n            lines = [next(f) for _ in range(10)]\n            print(\"\\nFirst 10 lines of file:\")\n            for i, line in enumerate(lines, 1):\n                print(f\"  {i:2d}: {line[:80]}\")\n    \nexcept FileNotFoundError:\n    print(\"❌ Error: Could not find the data file!\")\n    print(\"\\n📋 TO FIX THIS:\")\n    print(\"1. Click 'Add Data' button on the right sidebar\")\n    print(\"2. Search for: 'kepler exoplanet search results'\")\n    print(\"3. Look for the dataset by 'NASA' or one with ~9000 rows\")\n    print(\"4. Click 'Add' to add it to your notebook\")\n    print(\"\\nOR manually upload your CSV file\")\n    raise\n    \nexcept Exception as e:\n    print(f\"❌ Error loading data: {str(e)}\")\n    print(\"\\nPossible issues:\")\n    print(\"  1. Wrong file format\")\n    print(\"  2. File path is incorrect\") \n    print(\"  3. File is corrupted\")\n    print(f\"\\nFile path: {CONFIG['kepler_path']}\")\n    raise\n\n# Display first few rows\nprint(\"\\n\" + \"=\"*70)\nprint(\"SAMPLE DATA (First 5 rows)\")\nprint(\"=\"*70)\ndisplay(df.head())\n\n# Display column names\nprint(\"\\n\" + \"=\"*70)\nprint(\"AVAILABLE COLUMNS\")\nprint(\"=\"*70)\nprint(\"\\nAll columns in the dataset:\")\nfor i, col in enumerate(df.columns, 1):\n    print(f\"{i:3d}. {col}\")\n\n# Show columns that might be useful for features\nprint(\"\\n\" + \"=\"*70)\nprint(\"POTENTIALLY USEFUL FEATURE COLUMNS\")\nprint(\"=\"*70)\nuseful_keywords = ['koi_period', 'koi_duration', 'koi_depth', 'koi_prad', \n                   'koi_teq', 'koi_insol', 'koi_steff', 'koi_srad', \n                   'koi_smass', 'koi_model', 'koi_count', 'koi_num']\nfor col in df.columns:\n    if any(keyword in col.lower() for keyword in useful_keywords):\n        non_null = df[col].notna().sum()\n        pct = (non_null / len(df)) * 100\n        print(f\"  {col:30s} - {non_null:5d} values ({pct:5.1f}%)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:29:18.404447Z","iopub.execute_input":"2025-10-04T20:29:18.405209Z","iopub.status.idle":"2025-10-04T20:29:18.551807Z","shell.execute_reply.started":"2025-10-04T20:29:18.405177Z","shell.execute_reply":"2025-10-04T20:29:18.551153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 4: DATA EXPLORATION\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"DATA EXPLORATION\")\nprint(\"=\"*70)\n\n# Basic statistics\nprint(\"\\n1. BASIC STATISTICS\")\nprint(\"-\" * 70)\nprint(f\"Total records: {len(df):,}\")\nprint(f\"Total features: {len(df.columns)}\")\nprint(f\"Duplicate rows: {df.duplicated().sum()}\")\n\n# Check target variable\ntarget_col = 'koi_disposition'\nif target_col in df.columns:\n    print(f\"\\n2. TARGET VARIABLE: {target_col}\")\n    print(\"-\" * 70)\n    print(df[target_col].value_counts())\n    print(f\"\\nClass distribution:\")\n    for label, count in df[target_col].value_counts().items():\n        pct = (count / len(df)) * 100\n        print(f\"  {label:20s}: {count:5d} ({pct:5.1f}%)\")\nelse:\n    print(f\"\\n❌ Warning: Target column '{target_col}' not found!\")\n\n# Missing values\nprint(f\"\\n3. MISSING VALUES\")\nprint(\"-\" * 70)\nmissing = df.isnull().sum()\nmissing = missing[missing > 0].sort_values(ascending=False)\nif len(missing) > 0:\n    print(f\"Columns with missing values: {len(missing)}\")\n    print(\"\\nTop 10 columns with missing data:\")\n    for col, count in missing.head(10).items():\n        pct = (count / len(df)) * 100\n        print(f\"  {col:30s}: {count:6d} ({pct:5.1f}%)\")\nelse:\n    print(\"✓ No missing values in the dataset!\")\n\n# Data types\nprint(f\"\\n4. DATA TYPES\")\nprint(\"-\" * 70)\nprint(df.dtypes.value_counts())\n\n# Visualize target distribution\nif target_col in df.columns:\n    plt.figure(figsize=(10, 6))\n    df[target_col].value_counts().plot(kind='bar', color=['#2ecc71', '#f39c12', '#e74c3c'])\n    plt.title('Distribution of Exoplanet Classifications', fontsize=16, fontweight='bold')\n    plt.xlabel('Classification', fontsize=12)\n    plt.ylabel('Count', fontsize=12)\n    plt.xticks(rotation=45)\n    plt.grid(axis='y', alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:29:24.945995Z","iopub.execute_input":"2025-10-04T20:29:24.946651Z","iopub.status.idle":"2025-10-04T20:29:25.166075Z","shell.execute_reply.started":"2025-10-04T20:29:24.946625Z","shell.execute_reply":"2025-10-04T20:29:25.165287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 5: FEATURE SELECTION\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"FEATURE SELECTION\")\nprint(\"=\"*70)\n\n# Define features to use\nFEATURES = {\n    'orbital_period': 'koi_period',\n    'transit_duration': 'koi_duration',\n    'transit_depth': 'koi_depth',\n    'planet_radius': 'koi_prad',\n    'equilibrium_temp': 'koi_teq',\n    'insolation_flux': 'koi_insol',\n    'stellar_temp': 'koi_steff',\n    'stellar_radius': 'koi_srad',\n    'stellar_mass': 'koi_smass',\n    # Add more features that actually exist in the dataset\n    'period_error_1': 'koi_period_err1',\n    'period_error_2': 'koi_period_err2',\n    'duration_error_1': 'koi_duration_err1',\n    'duration_error_2': 'koi_duration_err2',\n    'depth_error_1': 'koi_depth_err1',\n    'depth_error_2': 'koi_depth_err2',\n    'prad_error_1': 'koi_prad_err1',\n    'prad_error_2': 'koi_prad_err2',\n    'model_snr': 'koi_model_snr',\n}\n\n# Check which features are available\navailable_features = {}\nmissing_features = []\n\nprint(\"\\nChecking feature availability:\")\nfor name, col in FEATURES.items():\n    if col in df.columns:\n        available_features[name] = col\n        print(f\"  ✓ {name:20s} -> {col}\")\n    else:\n        missing_features.append(name)\n        print(f\"  ✗ {name:20s} -> {col} (NOT FOUND)\")\n\nif missing_features:\n    print(f\"\\n⚠ Warning: {len(missing_features)} features not found: {missing_features}\")\n\n# Use available features\nfeature_cols = list(available_features.values())\nprint(f\"\\n✓ Using {len(feature_cols)} features for training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:29:31.344750Z","iopub.execute_input":"2025-10-04T20:29:31.345571Z","iopub.status.idle":"2025-10-04T20:29:31.352547Z","shell.execute_reply.started":"2025-10-04T20:29:31.345540Z","shell.execute_reply":"2025-10-04T20:29:31.351755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 6: DATA PREPROCESSING\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"DATA PREPROCESSING\")\nprint(\"=\"*70)\n\n# Extract features and target\nprint(\"\\n[1/6] Extracting features and target...\")\nX = df[feature_cols].copy()\ny = df[target_col].copy()\nprint(f\"  Feature matrix: {X.shape}\")\nprint(f\"  Target vector: {y.shape}\")\n\n# Remove rows with missing target\nprint(\"\\n[2/6] Removing rows with missing target...\")\nvalid_idx = y.notna()\nX = X[valid_idx]\ny = y[valid_idx]\nprint(f\"  Removed {(~valid_idx).sum()} rows\")\nprint(f\"  Remaining: {len(X):,} samples\")\n\n# Handle missing values in features\nprint(\"\\n[3/6] Handling missing values in features...\")\nmissing_before = X.isnull().sum().sum()\nprint(f\"  Missing values before: {missing_before:,}\")\n\nfor col in X.columns:\n    if X[col].isnull().any():\n        median_val = X[col].median()\n        X[col].fillna(median_val, inplace=True)\n        print(f\"    Filled {col} with median: {median_val:.2f}\")\n\nmissing_after = X.isnull().sum().sum()\nprint(f\"  Missing values after: {missing_after}\")\n\n# Standardize target labels\nprint(\"\\n[4/6] Standardizing target labels...\")\nprint(f\"  Original labels: {y.unique()}\")\n\n# Keep only valid labels\nvalid_labels = ['CONFIRMED', 'CANDIDATE', 'FALSE POSITIVE']\nvalid_mask = y.isin(valid_labels)\nX = X[valid_mask]\ny = y[valid_mask]\n\nprint(f\"  Standardized labels: {y.unique()}\")\nprint(f\"  Final dataset size: {len(X):,} samples\")\n\n# Encode target labels\nprint(\"\\n[5/6] Encoding target labels...\")\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\nprint(\"  Label mapping:\")\nfor i, label in enumerate(le.classes_):\n    count = (y_encoded == i).sum()\n    pct = (count / len(y_encoded)) * 100\n    print(f\"    {i} = {label:20s} ({count:5d} samples, {pct:5.1f}%)\")\n\n# Check class imbalance\nprint(\"\\n[6/6] Checking class imbalance...\")\nclass_counts = np.bincount(y_encoded)\nimbalance_ratio = class_counts.max() / class_counts.min()\nprint(f\"  Class distribution: {class_counts}\")\nprint(f\"  Imbalance ratio: {imbalance_ratio:.2f}:1\")\n\nif imbalance_ratio > 3:\n    print(\"  ⚠ Significant imbalance detected! SMOTE recommended.\")\nelse:\n    print(\"  ✓ Classes are relatively balanced\")\n\n# Display feature statistics\nprint(\"\\n\" + \"=\"*70)\nprint(\"FEATURE STATISTICS\")\nprint(\"=\"*70)\ndisplay(X.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:29:36.214631Z","iopub.execute_input":"2025-10-04T20:29:36.214945Z","iopub.status.idle":"2025-10-04T20:29:36.291101Z","shell.execute_reply.started":"2025-10-04T20:29:36.214901Z","shell.execute_reply":"2025-10-04T20:29:36.290441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 7: SPLIT DATA\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"SPLITTING DATA\")\nprint(\"=\"*70)\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_encoded,\n    test_size=CONFIG['test_size'],\n    random_state=CONFIG['random_state'],\n    stratify=y_encoded\n)\n\nprint(f\"Training set: {X_train.shape[0]:,} samples ({(1-CONFIG['test_size'])*100:.0f}%)\")\nprint(f\"Test set:     {X_test.shape[0]:,} samples ({CONFIG['test_size']*100:.0f}%)\")\nprint(f\"\\nFeatures: {X_train.shape[1]}\")\n\n# Display class distribution in splits\nprint(\"\\nClass distribution in splits:\")\nprint(f\"  Training: {np.bincount(y_train)}\")\nprint(f\"  Test:     {np.bincount(y_test)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:29:42.015571Z","iopub.execute_input":"2025-10-04T20:29:42.015883Z","iopub.status.idle":"2025-10-04T20:29:42.028724Z","shell.execute_reply.started":"2025-10-04T20:29:42.015862Z","shell.execute_reply":"2025-10-04T20:29:42.027866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # ====================================================================\n# CELL 8: FEATURE SCALING\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"FEATURE SCALING\")\nprint(\"=\"*70)\n\n# Initialize scaler\nscaler = StandardScaler()\n\n# Fit on training data and transform both sets\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"✓ Features scaled using StandardScaler\")\nprint(f\"  Training mean: {X_train_scaled.mean():.6f}\")\nprint(f\"  Training std:  {X_train_scaled.std():.6f}\")\nprint(f\"  Test mean:     {X_test_scaled.mean():.6f}\")\nprint(f\"  Test std:      {X_test_scaled.std():.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:29:45.194144Z","iopub.execute_input":"2025-10-04T20:29:45.194477Z","iopub.status.idle":"2025-10-04T20:29:45.298338Z","shell.execute_reply.started":"2025-10-04T20:29:45.194453Z","shell.execute_reply":"2025-10-04T20:29:45.297615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 9: HANDLE CLASS IMBALANCE (Using class_weight instead of SMOTE)\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"CLASS IMBALANCE HANDLING\")\nprint(\"=\"*70)\n\nprint(f\"Class distribution: {np.bincount(y_train)}\")\nprint(\"\\n✓ Using class_weight='balanced' in Random Forest\")\nprint(\"  This automatically adjusts weights inversely proportional to class frequencies\")\nprint(\"  More effective and faster than SMOTE for Random Forest!\")\n\n# No SMOTE needed - just use original data\nX_train_balanced = X_train_scaled\ny_train_balanced = y_train\n\nprint(f\"\\nTraining set size: {len(X_train_balanced):,} samples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:29:49.114081Z","iopub.execute_input":"2025-10-04T20:29:49.114703Z","iopub.status.idle":"2025-10-04T20:29:49.120002Z","shell.execute_reply.started":"2025-10-04T20:29:49.114674Z","shell.execute_reply":"2025-10-04T20:29:49.119083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 10: TRAIN RANDOM FOREST MODEL\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING RANDOM FOREST MODEL\")\nprint(\"=\"*70)\n\nprint(\"\\nModel parameters:\")\nprint(f\"  n_estimators: {CONFIG['n_estimators']}\")\nprint(f\"  max_depth: {CONFIG['max_depth']}\")\nprint(f\"  min_samples_split: {CONFIG['min_samples_split']}\")\nprint(f\"  min_samples_leaf: {CONFIG['min_samples_leaf']}\")\n\n# Initialize model with improved parameters\nmodel = RandomForestClassifier(\n    n_estimators=300,  # Increased from 200 for better performance\n    max_depth=25,      # Increased depth for more complex patterns\n    min_samples_split=10,  # More conservative splitting\n    min_samples_leaf=4,    # Prevent overfitting\n    max_features='sqrt',   # Use sqrt of features for each tree\n    class_weight='balanced',  # Handle class imbalance better\n    random_state=CONFIG['random_state'],\n    n_jobs=-1,\n    verbose=1\n)\n\n# Train model\nprint(\"\\n🚀 Training started...\")\nimport time\nstart_time = time.time()\n\nmodel.fit(X_train_balanced, y_train_balanced)\n\ntraining_time = time.time() - start_time\nprint(f\"\\n✓ Training complete in {training_time:.2f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:29:52.014138Z","iopub.execute_input":"2025-10-04T20:29:52.014774Z","iopub.status.idle":"2025-10-04T20:29:54.740932Z","shell.execute_reply.started":"2025-10-04T20:29:52.014746Z","shell.execute_reply":"2025-10-04T20:29:54.740039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ====================================================================\n# NEW CELL: FEATURE ENGINEERING\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ADVANCED FEATURE ENGINEERING\")\nprint(\"=\"*70)\n\ndef engineer_features(X_df):\n    \"\"\"\n    Create advanced features that help distinguish exoplanets\n    \"\"\"\n    X_new = X_df.copy()\n    \n    # 1. Ratio features (very important!)\n    print(\"\\n[1/4] Creating ratio features...\")\n    if 'koi_depth' in X_new.columns and 'koi_duration' in X_new.columns:\n        X_new['depth_to_duration_ratio'] = X_new['koi_depth'] / (X_new['koi_duration'] + 1e-6)\n        print(\"  ✓ depth_to_duration_ratio\")\n    \n    if 'koi_prad' in X_new.columns and 'koi_period' in X_new.columns:\n        X_new['radius_to_period_ratio'] = X_new['koi_prad'] / (X_new['koi_period'] + 1e-6)\n        print(\"  ✓ radius_to_period_ratio\")\n    \n    if 'koi_insol' in X_new.columns and 'koi_teq' in X_new.columns:\n        X_new['insol_to_temp_ratio'] = X_new['koi_insol'] / (X_new['koi_teq'] + 1e-6)\n        print(\"  ✓ insol_to_temp_ratio\")\n    \n    # 2. Polynomial features (capture non-linear relationships)\n    print(\"\\n[2/4] Creating polynomial features...\")\n    if 'koi_period' in X_new.columns:\n        X_new['period_squared'] = X_new['koi_period'] ** 2\n        X_new['period_log'] = np.log1p(X_new['koi_period'])\n        print(\"  ✓ period_squared, period_log\")\n    \n    if 'koi_depth' in X_new.columns:\n        X_new['depth_squared'] = X_new['koi_depth'] ** 2\n        X_new['depth_log'] = np.log1p(X_new['koi_depth'])\n        print(\"  ✓ depth_squared, depth_log\")\n    \n    # 3. Interaction features\n    print(\"\\n[3/4] Creating interaction features...\")\n    if 'koi_period' in X_new.columns and 'koi_duration' in X_new.columns:\n        X_new['period_duration_product'] = X_new['koi_period'] * X_new['koi_duration']\n        print(\"  ✓ period_duration_product\")\n    \n    if 'koi_steff' in X_new.columns and 'koi_srad' in X_new.columns:\n        X_new['stellar_luminosity'] = (X_new['koi_steff'] ** 4) * (X_new['koi_srad'] ** 2)\n        print(\"  ✓ stellar_luminosity\")\n    \n    # 4. Signal quality indicators\n    print(\"\\n[4/4] Creating signal quality features...\")\n    if 'koi_depth_err1' in X_new.columns and 'koi_depth' in X_new.columns:\n        X_new['depth_signal_to_noise'] = X_new['koi_depth'] / (np.abs(X_new['koi_depth_err1']) + 1e-6)\n        print(\"  ✓ depth_signal_to_noise\")\n    \n    if 'koi_period_err1' in X_new.columns and 'koi_period' in X_new.columns:\n        X_new['period_signal_to_noise'] = X_new['koi_period'] / (np.abs(X_new['koi_period_err1']) + 1e-6)\n        print(\"  ✓ period_signal_to_noise\")\n    \n    print(f\"\\n✓ Feature engineering complete!\")\n    print(f\"  Original features: {len(X_df.columns)}\")\n    print(f\"  New features: {len(X_new.columns)}\")\n    print(f\"  Added: {len(X_new.columns) - len(X_df.columns)} engineered features\")\n    \n    return X_new\n\n# Apply feature engineering to both train and test sets\nprint(\"\\nApplying to training set...\")\nX_train_engineered = engineer_features(X_train)\n\nprint(\"\\nApplying to test set...\")\nX_test_engineered = engineer_features(X_test)\n\n# Scale the new features\nprint(\"\\nScaling engineered features...\")\nscaler_new = StandardScaler()\nX_train_scaled_new = scaler_new.fit_transform(X_train_engineered)\nX_test_scaled_new = scaler_new.transform(X_test_engineered)\n\nprint(\"✓ All features scaled\")\n\n\n# ====================================================================\n# NEW CELL: TRAIN IMPROVED RANDOM FOREST\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING IMPROVED RANDOM FOREST\")\nprint(\"=\"*70)\n\n# Better hyperparameters\nmodel_improved = RandomForestClassifier(\n    n_estimators=500,          # More trees for stability\n    max_depth=30,              # Deeper trees\n    min_samples_split=10,      # More conservative\n    min_samples_leaf=4,        # Prevent overfitting\n    max_features='sqrt',       # Use sqrt of features\n    class_weight='balanced',   # Handle imbalance\n    bootstrap=True,            # Use bootstrap samples\n    oob_score=True,           # Out-of-bag score\n    random_state=42,\n    n_jobs=-1,\n    verbose=1\n)\n\nprint(\"\\nTraining with engineered features...\")\nimport time\nstart = time.time()\nmodel_improved.fit(X_train_scaled_new, y_train)\ntraining_time = time.time() - start\n\nprint(f\"\\n✓ Training complete in {training_time:.2f} seconds\")\nprint(f\"✓ Out-of-bag score: {model_improved.oob_score_:.4f} ({model_improved.oob_score_*100:.2f}%)\")\n\n\n# ====================================================================\n# NEW CELL: EVALUATE IMPROVED MODEL\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"IMPROVED MODEL EVALUATION\")\nprint(\"=\"*70)\n\n# Predictions\ny_pred_improved = model_improved.predict(X_test_scaled_new)\ny_proba_improved = model_improved.predict_proba(X_test_scaled_new)\n\n# Metrics\nacc_improved = accuracy_score(y_test, y_pred_improved)\nprec_improved = precision_score(y_test, y_pred_improved, average='weighted')\nrec_improved = recall_score(y_test, y_pred_improved, average='weighted')\nf1_improved = f1_score(y_test, y_pred_improved, average='weighted')\n\nprint(\"\\n🎯 IMPROVED MODEL RESULTS:\")\nprint(\"=\"*70)\nprint(f\"Accuracy:  {acc_improved:.4f} ({acc_improved*100:.2f}%)\")\nprint(f\"Precision: {prec_improved:.4f} ({prec_improved*100:.2f}%)\")\nprint(f\"Recall:    {rec_improved:.4f} ({rec_improved*100:.2f}%)\")\nprint(f\"F1-Score:  {f1_improved:.4f} ({f1_improved*100:.2f}%)\")\n\nprint(\"\\n📊 COMPARISON:\")\nprint(\"=\"*70)\nprint(f\"Previous Accuracy: 77.05%\")\nprint(f\"Improved Accuracy: {acc_improved*100:.2f}%\")\nprint(f\"Improvement: +{(acc_improved*100 - 77.05):.2f}%\")\n\n# Detailed report\nprint(\"\\n\" + \"=\"*70)\nprint(\"DETAILED CLASSIFICATION REPORT\")\nprint(\"=\"*70)\nprint(classification_report(y_test, y_pred_improved, \n                          target_names=le.classes_,\n                          digits=4))\n\n# Confusion matrix\ncm_improved = confusion_matrix(y_test, y_pred_improved)\ncm_df_improved = pd.DataFrame(\n    cm_improved,\n    index=[f'True {label}' for label in le.classes_],\n    columns=[f'Pred {label}' for label in le.classes_]\n)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"CONFUSION MATRIX\")\nprint(\"=\"*70)\ndisplay(cm_df_improved)\n\n# Plot confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm_improved, annot=True, fmt='d', cmap='RdYlGn',\n            xticklabels=le.classes_,\n            yticklabels=le.classes_,\n            cbar_kws={'label': 'Count'})\nplt.title('Improved Model - Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\nplt.ylabel('True Label', fontsize=12, fontweight='bold')\nplt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.savefig('confusion_matrix_improved.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n# ====================================================================\n# NEW CELL: FEATURE IMPORTANCE (UPDATED)\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TOP FEATURES IN IMPROVED MODEL\")\nprint(\"=\"*70)\n\n# Get feature importances\nfeature_importance_improved = pd.DataFrame({\n    'feature': X_train_engineered.columns,\n    'importance': model_improved.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nTop 15 Most Important Features:\")\nprint(\"-\" * 70)\nfor idx, row in feature_importance_improved.head(15).iterrows():\n    bar = '█' * int(row['importance'] * 100)\n    print(f\"  {row['feature']:35s}: {row['importance']:.4f} {bar}\")\n\n# Plot\nplt.figure(figsize=(12, 8))\ntop_features = feature_importance_improved.head(15)\nplt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\nplt.yticks(range(len(top_features)), top_features['feature'])\nplt.xlabel('Importance', fontsize=12, fontweight='bold')\nplt.title('Top 15 Feature Importances (Improved Model)', fontsize=16, fontweight='bold', pad=20)\nplt.gca().invert_yaxis()\nplt.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.savefig('feature_importance_improved.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n# ====================================================================\n# NEW CELL: SAVE IMPROVED MODEL\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"SAVING IMPROVED MODEL\")\nprint(\"=\"*70)\n\n# Save all components\njoblib.dump(model_improved, 'exoplanet_detector_improved_model.pkl')\njoblib.dump(scaler_new, 'exoplanet_detector_improved_scaler.pkl')\njoblib.dump(le, 'exoplanet_detector_improved_label_encoder.pkl')\njoblib.dump(X_train_engineered.columns.tolist(), 'exoplanet_detector_improved_features.pkl')\n\n# Save metadata\nmetadata_improved = {\n    'test_accuracy': float(acc_improved),\n    'test_precision': float(prec_improved),\n    'test_recall': float(rec_improved),\n    'test_f1': float(f1_improved),\n    'oob_score': float(model_improved.oob_score_),\n    'n_features': len(X_train_engineered.columns),\n    'n_original_features': 17,\n    'n_engineered_features': len(X_train_engineered.columns) - 17,\n    'n_estimators': 500,\n    'training_time': training_time,\n    'model_type': 'Random Forest with Feature Engineering',\n}\n\nimport json\nwith open('exoplanet_detector_improved_metadata.json', 'w') as f:\n    json.dump(metadata_improved, f, indent=2)\n\nprint(\"✓ Improved model saved:\")\nprint(\"  - exoplanet_detector_improved_model.pkl\")\nprint(\"  - exoplanet_detector_improved_scaler.pkl\")\nprint(\"  - exoplanet_detector_improved_label_encoder.pkl\")\nprint(\"  - exoplanet_detector_improved_features.pkl\")\nprint(\"  - exoplanet_detector_improved_metadata.json\")\n\n\n# ====================================================================\n# NEW CELL: FINAL COMPARISON\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL MODEL COMPARISON\")\nprint(\"=\"*70)\n\ncomparison = pd.DataFrame({\n    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Features', 'Training Time'],\n    'Original Model': ['77.05%', '78.14%', '77.05%', '77.47%', '17', '3.08s'],\n    'Improved Model': [\n        f'{acc_improved*100:.2f}%',\n        f'{prec_improved*100:.2f}%',\n        f'{rec_improved*100:.2f}%',\n        f'{f1_improved*100:.2f}%',\n        str(len(X_train_engineered.columns)),\n        f'{training_time:.2f}s'\n    ],\n    'Improvement': [\n        f'+{(acc_improved*100 - 77.05):.2f}%',\n        f'+{(prec_improved*100 - 78.14):.2f}%',\n        f'+{(rec_improved*100 - 77.05):.2f}%',\n        f'+{(f1_improved*100 - 77.47):.2f}%',\n        f'+{len(X_train_engineered.columns) - 17}',\n        f'+{training_time - 3.08:.2f}s'\n    ]\n})\n\ndisplay(comparison)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"🎉 IMPROVEMENT COMPLETE!\")\nprint(\"=\"*70)\nprint(f\"\"\"\nKey Improvements:\n  ✓ Accuracy improved from 77.05% to {acc_improved*100:.2f}%\n  ✓ Added {len(X_train_engineered.columns) - 17} engineered features\n  ✓ Increased model complexity (500 trees)\n  ✓ Better hyperparameters\n  ✓ Out-of-bag validation: {model_improved.oob_score_*100:.2f}%\n\nNext Steps:\n  1. Download the improved model files\n  2. Use in production/web app\n  3. Consider trying XGBoost for even better results\n  4. Deploy to Flask API\n\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:29:58.454529Z","iopub.execute_input":"2025-10-04T20:29:58.454819Z","iopub.status.idle":"2025-10-04T20:30:07.007761Z","shell.execute_reply.started":"2025-10-04T20:29:58.454800Z","shell.execute_reply":"2025-10-04T20:30:07.007065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 11: EVALUATE MODEL\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODEL EVALUATION\")\nprint(\"=\"*70)\n\n# Training set evaluation\nprint(\"\\n1. TRAINING SET PERFORMANCE\")\nprint(\"-\" * 70)\ny_train_pred = model.predict(X_train_scaled)\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\nprint(f\"Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n\n# Test set evaluation\nprint(\"\\n2. TEST SET PERFORMANCE\")\nprint(\"-\" * 70)\ny_test_pred = model.predict(X_test_scaled)\ny_test_proba = model.predict_proba(X_test_scaled)\n\ntest_accuracy = accuracy_score(y_test, y_test_pred)\ntest_precision = precision_score(y_test, y_test_pred, average='weighted')\ntest_recall = recall_score(y_test, y_test_pred, average='weighted')\ntest_f1 = f1_score(y_test, y_test_pred, average='weighted')\n\nprint(f\"Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\nprint(f\"Precision: {test_precision:.4f} ({test_precision*100:.2f}%)\")\nprint(f\"Recall:    {test_recall:.4f} ({test_recall*100:.2f}%)\")\nprint(f\"F1-Score:  {test_f1:.4f} ({test_f1*100:.2f}%)\")\n\n# Detailed classification report\nprint(\"\\n3. DETAILED CLASSIFICATION REPORT\")\nprint(\"-\" * 70)\nprint(classification_report(y_test, y_test_pred, \n                          target_names=le.classes_,\n                          digits=4))\n\n# Confusion matrix\nprint(\"\\n4. CONFUSION MATRIX\")\nprint(\"-\" * 70)\ncm = confusion_matrix(y_test, y_test_pred)\ncm_df = pd.DataFrame(\n    cm,\n    index=[f'True {label}' for label in le.classes_],\n    columns=[f'Pred {label}' for label in le.classes_]\n)\ndisplay(cm_df)\n\n# Plot confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=le.classes_,\n            yticklabels=le.classes_,\n            cbar_kws={'label': 'Count'})\nplt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold', pad=20)\nplt.ylabel('True Label', fontsize=12, fontweight='bold')\nplt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n✓ Confusion matrix saved as 'confusion_matrix.png'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:30:12.740125Z","iopub.execute_input":"2025-10-04T20:30:12.740451Z","iopub.status.idle":"2025-10-04T20:30:13.922727Z","shell.execute_reply.started":"2025-10-04T20:30:12.740428Z","shell.execute_reply":"2025-10-04T20:30:13.921968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ====================================================================\n# CELL 12: FEATURE IMPORTANCE\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"FEATURE IMPORTANCE ANALYSIS\")\nprint(\"=\"*70)\n\n# Get feature importances\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nTop 10 Most Important Features:\")\nprint(\"-\" * 70)\nfor idx, row in feature_importance.head(10).iterrows():\n    print(f\"  {row['feature']:30s}: {row['importance']:.4f}\")\n\n# Plot feature importance\nplt.figure(figsize=(10, 8))\ntop_features = feature_importance.head(10)\nplt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\nplt.yticks(range(len(top_features)), top_features['feature'])\nplt.xlabel('Importance', fontsize=12, fontweight='bold')\nplt.title('Top 10 Feature Importances', fontsize=16, fontweight='bold', pad=20)\nplt.gca().invert_yaxis()\nplt.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n✓ Feature importance plot saved as 'feature_importance.png'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:30:22.195385Z","iopub.execute_input":"2025-10-04T20:30:22.195696Z","iopub.status.idle":"2025-10-04T20:30:22.959454Z","shell.execute_reply.started":"2025-10-04T20:30:22.195674Z","shell.execute_reply":"2025-10-04T20:30:22.958658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 13: CROSS-VALIDATION\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"CROSS-VALIDATION\")\nprint(\"=\"*70)\n\nprint(\"Performing 5-fold cross-validation...\")\ncv_scores = cross_val_score(\n    model, X_train_balanced, y_train_balanced,\n    cv=5, scoring='accuracy', n_jobs=-1\n)\n\nprint(\"\\nCross-validation scores:\")\nfor i, score in enumerate(cv_scores, 1):\n    print(f\"  Fold {i}: {score:.4f} ({score*100:.2f}%)\")\n\nprint(f\"\\nMean CV Score: {cv_scores.mean():.4f} (± {cv_scores.std():.4f})\")\nprint(f\"Min CV Score:  {cv_scores.min():.4f}\")\nprint(f\"Max CV Score:  {cv_scores.max():.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 14: SAVE MODEL\n# ====================================================================\n\nif CONFIG['save_model']:\n    print(\"\\n\" + \"=\"*70)\n    print(\"SAVING MODEL\")\n    print(\"=\"*70)\n    \n    model_prefix = CONFIG['model_name']\n    \n    # Save model components\n    joblib.dump(model, f'{model_prefix}_model.pkl')\n    print(f\"✓ Model saved: {model_prefix}_model.pkl\")\n    \n    joblib.dump(scaler, f'{model_prefix}_scaler.pkl')\n    print(f\"✓ Scaler saved: {model_prefix}_scaler.pkl\")\n    \n    joblib.dump(le, f'{model_prefix}_label_encoder.pkl')\n    print(f\"✓ Label encoder saved: {model_prefix}_label_encoder.pkl\")\n    \n    joblib.dump(feature_cols, f'{model_prefix}_features.pkl')\n    print(f\"✓ Features saved: {model_prefix}_features.pkl\")\n    \n    # Save metadata\n    metadata = {\n        'train_accuracy': float(train_accuracy),\n        'test_accuracy': float(test_accuracy),\n        'precision': float(test_precision),\n        'recall': float(test_recall),\n        'f1_score': float(test_f1),\n        'cv_mean': float(cv_scores.mean()),\n        'cv_std': float(cv_scores.std()),\n        'n_features': len(feature_cols),\n        'n_train_samples': len(X_train),\n        'n_test_samples': len(X_test),\n        'training_time': training_time,\n        'config': CONFIG\n    }\n    \n    import json\n    with open(f'{model_prefix}_metadata.json', 'w') as f:\n        json.dump(metadata, f, indent=2)\n    print(f\"✓ Metadata saved: {model_prefix}_metadata.json\")\n    \n    print(f\"\\n✓ All model files saved with prefix: {model_prefix}_*\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:30:27.724277Z","iopub.execute_input":"2025-10-04T20:30:27.724589Z","iopub.status.idle":"2025-10-04T20:30:27.968552Z","shell.execute_reply.started":"2025-10-04T20:30:27.724567Z","shell.execute_reply":"2025-10-04T20:30:27.967439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ====================================================================\n# CELL 15: TEST PREDICTIONS\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TESTING PREDICTIONS\")\nprint(\"=\"*70)\n\n# Example: Predict on first 5 test samples\nprint(\"\\nExample predictions on test set:\")\nprint(\"-\" * 70)\n\nfor i in range(min(5, len(X_test))):\n    sample = X_test_scaled[i:i+1]\n    pred_class = model.predict(sample)[0]\n    pred_proba = model.predict_proba(sample)[0]\n    true_class = y_test.iloc[i] if hasattr(y_test, 'iloc') else y_test[i]\n    \n    pred_label = le.inverse_transform([pred_class])[0]\n    true_label = le.inverse_transform([true_class])[0]\n    \n    print(f\"\\nSample {i+1}:\")\n    print(f\"  True label: {true_label}\")\n    print(f\"  Predicted:  {pred_label}\")\n    print(f\"  Confidence: {pred_proba[pred_class]*100:.2f}%\")\n    print(f\"  Probabilities:\")\n    for j, label in enumerate(le.classes_):\n        print(f\"    {label:20s}: {pred_proba[j]*100:5.2f}%\")\n    print(f\"  {'✓ CORRECT' if pred_label == true_label else '✗ WRONG'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:30:38.980432Z","iopub.execute_input":"2025-10-04T20:30:38.980733Z","iopub.status.idle":"2025-10-04T20:30:39.554414Z","shell.execute_reply.started":"2025-10-04T20:30:38.980713Z","shell.execute_reply":"2025-10-04T20:30:39.553592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ====================================================================\n# CELL 16: PREDICTION FUNCTION\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"CREATING PREDICTION FUNCTION\")\nprint(\"=\"*70)\n\ndef predict_exoplanet(input_data):\n    \"\"\"\n    Predict exoplanet classification from input features\n    \n    Parameters:\n    -----------\n    input_data : dict or list\n        Dictionary with feature names as keys, or list of values\n        \n    Returns:\n    --------\n    dict : Prediction results with classification and probabilities\n    \"\"\"\n    # Convert input to DataFrame\n    if isinstance(input_data, dict):\n        df_input = pd.DataFrame([input_data])\n    else:\n        df_input = pd.DataFrame([input_data], columns=feature_cols)\n    \n    # Ensure correct column order\n    df_input = df_input[feature_cols]\n    \n    # Scale features\n    input_scaled = scaler.transform(df_input)\n    \n    # Make prediction\n    prediction = model.predict(input_scaled)[0]\n    probabilities = model.predict_proba(input_scaled)[0]\n    \n    # Get label\n    predicted_label = le.inverse_transform([prediction])[0]\n    confidence = probabilities[prediction] * 100\n    \n    # Return results\n    return {\n        'classification': predicted_label,\n        'confidence': confidence,\n        'probabilities': {\n            le.classes_[i]: prob * 100 \n            for i, prob in enumerate(probabilities)\n        }\n    }\n\n# Test the function\nprint(\"\\nTesting prediction function...\")\n\n# Create test input with ALL the features that were actually used\n# Check how many features were used\nprint(f\"Model expects {len(feature_cols)} features: {feature_cols[:5]}... (showing first 5)\")\n\n# Create a complete test input matching the actual features used\ntest_input = {}\nfor col in feature_cols:\n    # Set reasonable default values for each feature\n    if 'period' in col:\n        test_input[col] = 10.5 if 'err' not in col else 0.1\n    elif 'duration' in col:\n        test_input[col] = 3.2 if 'err' not in col else 0.1\n    elif 'depth' in col:\n        test_input[col] = 500 if 'err' not in col else 10\n    elif 'prad' in col:\n        test_input[col] = 5.4 if 'err' not in col else 0.2\n    elif 'teq' in col:\n        test_input[col] = 500\n    elif 'insol' in col:\n        test_input[col] = 1.5\n    elif 'steff' in col:\n        test_input[col] = 5500\n    elif 'srad' in col:\n        test_input[col] = 1.2\n    elif 'smass' in col:\n        test_input[col] = 1.0\n    elif 'snr' in col:\n        test_input[col] = 15.0\n    else:\n        test_input[col] = 1.0  # Default value\n\nresult = predict_exoplanet(test_input)\nprint(f\"\\nTest Prediction:\")\nprint(f\"  Classification: {result['classification']}\")\nprint(f\"  Confidence: {result['confidence']:.2f}%\")\nprint(f\"  Probabilities:\")\nfor label, prob in result['probabilities'].items():\n    print(f\"    {label:20s}: {prob:5.2f}%\")\n\nprint(\"\\n✓ Prediction function ready to use!\")\nprint(\"\\nTo make predictions, provide a dictionary with these features:\")\nprint(f\"  Required features: {feature_cols}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:30:57.479445Z","iopub.execute_input":"2025-10-04T20:30:57.479750Z","iopub.status.idle":"2025-10-04T20:30:57.595460Z","shell.execute_reply.started":"2025-10-04T20:30:57.479727Z","shell.execute_reply":"2025-10-04T20:30:57.594556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ====================================================================\n# CELL 17: SUMMARY REPORT\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL SUMMARY REPORT\")\nprint(\"=\"*70)\n\nprint(f\"\"\"\n╔══════════════════════════════════════════════════════════════════╗\n║              EXOPLANET DETECTION MODEL - SUMMARY                 ║\n╚══════════════════════════════════════════════════════════════════╝\n\n📊 DATASET INFORMATION\n  • Total samples: {len(df):,}\n  • Training samples: {len(X_train):,}\n  • Test samples: {len(X_test):,}\n  • Features used: {len(feature_cols)}\n  • Classes: {len(le.classes_)}\n\n🎯 MODEL PERFORMANCE\n  • Test Accuracy:  {test_accuracy*100:.2f}%\n  • Precision:      {test_precision*100:.2f}%\n  • Recall:         {test_recall*100:.2f}%\n  • F1-Score:       {test_f1*100:.2f}%\n  • CV Score:       {cv_scores.mean()*100:.2f}% (± {cv_scores.std()*100:.2f}%)\n\n🌲 MODEL DETAILS\n  • Algorithm: Random Forest\n  • Trees: {CONFIG['n_estimators']}\n  • Max depth: {CONFIG['max_depth']}\n  • Training time: {training_time:.2f} seconds\n\n📁 SAVED FILES\n  • {model_prefix}_model.pkl\n  • {model_prefix}_scaler.pkl\n  • {model_prefix}_label_encoder.pkl\n  • {model_prefix}_features.pkl\n  • {model_prefix}_metadata.json\n  • confusion_matrix.png\n  • feature_importance.png\n\n✅ MODEL STATUS: READY FOR DEPLOYMENT!\n\"\"\")\n\nprint(\"=\"*70)\nprint(\"✓ TRAINING COMPLETE!\")\nprint(\"=\"*70)\nprint(\"\\nYour model is trained and ready to use!\")\nprint(\"Download the .pkl files to use in production.\")\nprint(\"\\nNext steps:\")\nprint(\"  1. Download model files from Kaggle output\")\nprint(\"  2. Create Flask API for predictions\")\nprint(\"  3. Connect to your web interface\")\nprint(\"  4. Deploy to production!\")\n\n\n# ====================================================================\n# END OF NOTEBOOK\n# ====================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:31:14.599004Z","iopub.execute_input":"2025-10-04T20:31:14.599662Z","iopub.status.idle":"2025-10-04T20:31:14.614440Z","shell.execute_reply.started":"2025-10-04T20:31:14.599633Z","shell.execute_reply":"2025-10-04T20:31:14.613498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL: DATA PREPROCESSING (FOR XGBOOST)\n# ====================================================================\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pandas as pd\nimport numpy as np\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"DATA PREPROCESSING\")\nprint(\"=\"*70)\n\n# Example: if your dataset is named 'df'\n# and has a target column called 'LABEL' (change it accordingly)\nX = df.drop('LABEL', axis=1)\ny = df['LABEL']\n\n# Encode labels if not numeric\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n# Split into train/test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Feature scaling\nscaler_new = StandardScaler()\nX_train_scaled_new = scaler_new.fit_transform(X_train)\nX_test_scaled_new = scaler_new.transform(X_test)\n\n# Convert to DataFrame for feature naming\nX_train_scaled_new = pd.DataFrame(X_train_scaled_new, columns=X_train.columns)\nX_test_scaled_new = pd.DataFrame(X_test_scaled_new, columns=X_test.columns)\n\n# Feature-engineered version (if needed)\nX_train_engineered = X_train_scaled_new.copy()\nX_test_engineered = X_test_scaled_new.copy()\n\nprint(\"\\n✓ Data preprocessing complete!\")\nprint(f\"  Training samples: {X_train.shape[0]}\")\nprint(f\"  Testing samples:  {X_test.shape[0]}\")\nprint(f\"  Total features:   {X_train.shape[1]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n====================================================================\nXGBOOST MODEL FOR EXOPLANET DETECTION\nExpected: 85-92% Accuracy\n====================================================================\n\nAdd these cells to your Kaggle notebook\n\"\"\"\n\n# ====================================================================\n# CELL: INSTALL XGBOOST\n# ====================================================================\n\n#!pip install xgboost -q\n#print(\"✓ XGBoost installed\")\n\n\n# ====================================================================\n# CELL: TRAIN XGBOOST MODEL\n# ====================================================================\n\nimport xgboost as xgb\nimport time\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING XGBOOST MODEL\")\nprint(\"=\"*70)\n\n# XGBoost with optimized hyperparameters\nxgb_model = xgb.XGBClassifier(\n    n_estimators=500,           # Number of boosting rounds\n    max_depth=8,                # Tree depth\n    learning_rate=0.05,         # Learning rate (smaller = more accurate)\n    subsample=0.8,              # Subsample ratio\n    colsample_bytree=0.8,       # Feature sampling\n    min_child_weight=3,         # Minimum sum of instance weight\n    gamma=0.1,                  # Minimum loss reduction\n    reg_alpha=0.1,              # L1 regularization\n    reg_lambda=1.0,             # L2 regularization\n    scale_pos_weight=1,         # Balance classes\n    objective='multi:softprob', # Multi-class probability\n    eval_metric='mlogloss',     # Evaluation metric\n    random_state=42,\n    n_jobs=-1,\n    verbosity=1\n)\n\nprint(\"\\nModel parameters:\")\nprint(f\"  n_estimators: {xgb_model.n_estimators}\")\nprint(f\"  max_depth: {xgb_model.max_depth}\")\nprint(f\"  learning_rate: {xgb_model.learning_rate}\")\n\nprint(\"\\n🚀 Training XGBoost...\")\nstart = time.time()\n\n# Train with evaluation set\nxgb_model.fit(\n    X_train_scaled_new, y_train,\n    eval_set=[(X_test_scaled_new, y_test)],\n    verbose=50  # Print every 50 rounds\n)\n\nxgb_training_time = time.time() - start\nprint(f\"\\n✓ Training complete in {xgb_training_time:.2f} seconds\")\n\n\n# ====================================================================\n# CELL: EVALUATE XGBOOST\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"XGBOOST MODEL EVALUATION\")\nprint(\"=\"*70)\n\n# Predictions\ny_pred_xgb = xgb_model.predict(X_test_scaled_new)\ny_proba_xgb = xgb_model.predict_proba(X_test_scaled_new)\n\n# Metrics\nacc_xgb = accuracy_score(y_test, y_pred_xgb)\nprec_xgb = precision_score(y_test, y_pred_xgb, average='weighted')\nrec_xgb = recall_score(y_test, y_pred_xgb, average='weighted')\nf1_xgb = f1_score(y_test, y_pred_xgb, average='weighted')\n\nprint(\"\\n🎯 XGBOOST RESULTS:\")\nprint(\"=\"*70)\nprint(f\"Accuracy:  {acc_xgb:.4f} ({acc_xgb*100:.2f}%)\")\nprint(f\"Precision: {prec_xgb:.4f} ({prec_xgb*100:.2f}%)\")\nprint(f\"Recall:    {rec_xgb:.4f} ({rec_xgb*100:.2f}%)\")\nprint(f\"F1-Score:  {f1_xgb:.4f} ({f1_xgb*100:.2f}%)\")\n\n# Detailed report\nprint(\"\\n\" + \"=\"*70)\nprint(\"DETAILED CLASSIFICATION REPORT\")\nprint(\"=\"*70)\nprint(classification_report(y_test, y_pred_xgb, \n                          target_names=le.classes_,\n                          digits=4))\n\n# Confusion matrix\ncm_xgb = confusion_matrix(y_test, y_pred_xgb)\ncm_df_xgb = pd.DataFrame(\n    cm_xgb,\n    index=[f'True {label}' for label in le.classes_],\n    columns=[f'Pred {label}' for label in le.classes_]\n)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"CONFUSION MATRIX\")\nprint(\"=\"*70)\ndisplay(cm_df_xgb)\n\n# Plot\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm_xgb, annot=True, fmt='d', cmap='YlOrRd',\n            xticklabels=le.classes_,\n            yticklabels=le.classes_)\nplt.title('XGBoost - Confusion Matrix', fontsize=16, fontweight='bold')\nplt.ylabel('True Label', fontsize=12)\nplt.xlabel('Predicted Label', fontsize=12)\nplt.tight_layout()\nplt.savefig('confusion_matrix_xgboost.png', dpi=300)\nplt.show()\n\n\n# ====================================================================\n# CELL: XGBOOST FEATURE IMPORTANCE\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"XGBOOST FEATURE IMPORTANCE\")\nprint(\"=\"*70)\n\n# Get feature importances\nxgb_importance = pd.DataFrame({\n    'feature': X_train_engineered.columns,\n    'importance': xgb_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nTop 15 Most Important Features:\")\nprint(\"-\" * 70)\nfor idx, row in xgb_importance.head(15).iterrows():\n    bar = '█' * int(row['importance'] * 100)\n    print(f\"  {row['feature']:35s}: {row['importance']:.4f} {bar}\")\n\n# Plot\nplt.figure(figsize=(12, 8))\ntop_15 = xgb_importance.head(15)\nplt.barh(range(len(top_15)), top_15['importance'], color='#FFA500')\nplt.yticks(range(len(top_15)), top_15['feature'])\nplt.xlabel('Importance', fontsize=12, fontweight='bold')\nplt.title('XGBoost - Top 15 Features', fontsize=16, fontweight='bold')\nplt.gca().invert_yaxis()\nplt.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.savefig('feature_importance_xgboost.png', dpi=300)\nplt.show()\n\n\n# ====================================================================\n# CELL: ENSEMBLE (Random Forest + XGBoost)\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ENSEMBLE: RANDOM FOREST + XGBOOST\")\nprint(\"=\"*70)\n\n# Weighted ensemble predictions\nprint(\"\\nCombining predictions with weighted average...\")\nprint(\"  Random Forest weight: 0.4\")\nprint(\"  XGBoost weight: 0.6\")\n\n# Get probability predictions from both models\nrf_proba = model_improved.predict_proba(X_test_scaled_new)\nxgb_proba = xgb_model.predict_proba(X_test_scaled_new)\n\n# Weighted average\nensemble_proba = 0.4 * rf_proba + 0.6 * xgb_proba\nensemble_pred = np.argmax(ensemble_proba, axis=1)\n\n# Evaluate ensemble\nacc_ensemble = accuracy_score(y_test, ensemble_pred)\nprec_ensemble = precision_score(y_test, ensemble_pred, average='weighted')\nrec_ensemble = recall_score(y_test, ensemble_pred, average='weighted')\nf1_ensemble = f1_score(y_test, ensemble_pred, average='weighted')\n\nprint(\"\\n🎯 ENSEMBLE RESULTS:\")\nprint(\"=\"*70)\nprint(f\"Accuracy:  {acc_ensemble:.4f} ({acc_ensemble*100:.2f}%)\")\nprint(f\"Precision: {prec_ensemble:.4f} ({prec_ensemble*100:.2f}%)\")\nprint(f\"Recall:    {rec_ensemble:.4f} ({rec_ensemble*100:.2f}%)\")\nprint(f\"F1-Score:  {f1_ensemble:.4f} ({f1_ensemble*100:.2f}%)\")\n\n# Detailed report\nprint(\"\\n\" + \"=\"*70)\nprint(\"DETAILED CLASSIFICATION REPORT\")\nprint(\"=\"*70)\nprint(classification_report(y_test, ensemble_pred, \n                          target_names=le.classes_,\n                          digits=4))\n\n\n# ====================================================================\n# CELL: FINAL MODEL COMPARISON\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"🏆 FINAL MODEL COMPARISON\")\nprint(\"=\"*70)\n\ncomparison_final = pd.DataFrame({\n    'Model': [\n        'Original Random Forest',\n        'Improved Random Forest',\n        'XGBoost',\n        'Ensemble (RF + XGBoost)'\n    ],\n    'Accuracy': [\n        '77.05%',\n        '77.42%',\n        f'{acc_xgb*100:.2f}%',\n        f'{acc_ensemble*100:.2f}%'\n    ],\n    'Precision': [\n        '78.14%',\n        f'{prec_improved*100:.2f}%',\n        f'{prec_xgb*100:.2f}%',\n        f'{prec_ensemble*100:.2f}%'\n    ],\n    'F1-Score': [\n        '77.47%',\n        f'{f1_improved*100:.2f}%',\n        f'{f1_xgb*100:.2f}%',\n        f'{f1_ensemble*100:.2f}%'\n    ],\n    'Training Time': [\n        '3.08s',\n        f'{training_time:.2f}s',\n        f'{xgb_training_time:.2f}s',\n        'N/A (ensemble)'\n    ]\n})\n\ndisplay(comparison_final)\n\n# Visualization\nfig, ax = plt.subplots(figsize=(12, 6))\n\nmodels = ['Original RF', 'Improved RF', 'XGBoost', 'Ensemble']\naccuracies = [77.05, acc_improved*100, acc_xgb*100, acc_ensemble*100]\ncolors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n\nbars = ax.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n\n# Add value labels on bars\nfor bar, acc in zip(bars, accuracies):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{acc:.2f}%',\n            ha='center', va='bottom', fontsize=14, fontweight='bold')\n\nax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\nax.set_title('Model Accuracy Comparison', fontsize=16, fontweight='bold', pad=20)\nax.set_ylim([70, max(accuracies) + 5])\nax.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.savefig('model_comparison.png', dpi=300)\nplt.show()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"🎉 BEST MODEL SELECTED!\")\nprint(\"=\"*70)\n\nbest_models = ['XGBoost', 'Ensemble']\nbest_acc = max(acc_xgb, acc_ensemble)\nbest_name = 'XGBoost' if acc_xgb >= acc_ensemble else 'Ensemble'\n\nprint(f\"\"\"\n🏆 Winner: {best_name}\n  • Accuracy: {best_acc*100:.2f}%\n  • Improvement from original: +{(best_acc*100 - 77.05):.2f}%\n  \n✓ Model is ready for production!\n\"\"\")\n\n\n# ====================================================================\n# CELL: SAVE BEST MODEL\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"SAVING BEST MODEL (XGBOOST)\")\nprint(\"=\"*70)\n\n# Save XGBoost model\nxgb_model.save_model('exoplanet_xgboost_model.json')\nprint(\"✓ XGBoost model saved: exoplanet_xgboost_model.json\")\n\n# Save ensemble components\njoblib.dump({\n    'rf_model': model_improved,\n    'xgb_model': xgb_model,\n    'rf_weight': 0.4,\n    'xgb_weight': 0.6\n}, 'exoplanet_ensemble_model.pkl')\nprint(\"✓ Ensemble saved: exoplanet_ensemble_model.pkl\")\n\n# Save scaler and encoder (already saved, but for completeness)\njoblib.dump(scaler_new, 'exoplanet_final_scaler.pkl')\njoblib.dump(le, 'exoplanet_final_label_encoder.pkl')\njoblib.dump(X_train_engineered.columns.tolist(), 'exoplanet_final_features.pkl')\n\nprint(\"\\n✓ All models saved!\")\nprint(\"\\nFiles created:\")\nprint(\"  - exoplanet_xgboost_model.json\")\nprint(\"  - exoplanet_ensemble_model.pkl\")\nprint(\"  - exoplanet_final_scaler.pkl\")\nprint(\"  - exoplanet_final_label_encoder.pkl\")\nprint(\"  - exoplanet_final_features.pkl\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"✅ XGBOOST TRAINING COMPLETE!\")\nprint(\"=\"*70)\nprint(f\"\"\"\nReady for deployment! 🚀\n\nNext steps:\n  1. Download model files from Kaggle\n  2. Create Flask API for predictions\n  3. Connect to your React web interface\n  4. Deploy to production (Heroku, AWS, etc.)\n\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T20:31:28.665656Z","iopub.execute_input":"2025-10-04T20:31:28.665998Z","iopub.status.idle":"2025-10-04T20:31:37.845582Z","shell.execute_reply.started":"2025-10-04T20:31:28.665975Z","shell.execute_reply":"2025-10-04T20:31:37.844751Z"}},"outputs":[],"execution_count":null}]}